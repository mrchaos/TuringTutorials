{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we demonstrate how one can implement a Bayesian Neural Network using a combination of Turing and [Flux](https://github.com/FluxML/Flux.jl), a suite of machine learning tools. We will use Flux to specify the neural network's layers and Turing to implement the probabilistic inference, with the goal of implementing a classification algorithm.\n\nWe will begin with importing the relevant libraries."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Turing\nusing FillArrays\nusing Flux\nusing Plots\nusing ReverseDiff\n\nusing LinearAlgebra\nusing Random\n\n# Use reverse_diff due to the number of parameters in neural networks.\nTuring.setadbackend(:reversediff)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal here is to use a Bayesian neural network to classify points in an artificial dataset.\nThe code below generates data points arranged in a box-like pattern and displays a graph of the dataset we will be working with."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Number of points to generate.\nN = 80\nM = round(Int, N / 4)\nRandom.seed!(1234)\n\n# Generate artificial data.\nx1s = rand(M) * 4.5;\nx2s = rand(M) * 4.5;\nxt1s = Array([[x1s[i] + 0.5; x2s[i] + 0.5] for i in 1:M])\nx1s = rand(M) * 4.5;\nx2s = rand(M) * 4.5;\nappend!(xt1s, Array([[x1s[i] - 5; x2s[i] - 5] for i in 1:M]))\n\nx1s = rand(M) * 4.5;\nx2s = rand(M) * 4.5;\nxt0s = Array([[x1s[i] + 0.5; x2s[i] - 5] for i in 1:M])\nx1s = rand(M) * 4.5;\nx2s = rand(M) * 4.5;\nappend!(xt0s, Array([[x1s[i] - 5; x2s[i] + 0.5] for i in 1:M]))\n\n# Store all the data for later.\nxs = [xt1s; xt0s]\nts = [ones(2 * M); zeros(2 * M)]\n\n# Plot data points.\nfunction plot_data()\n    x1 = map(e -> e[1], xt1s)\n    y1 = map(e -> e[2], xt1s)\n    x2 = map(e -> e[1], xt0s)\n    y2 = map(e -> e[2], xt0s)\n\n    Plots.scatter(x1, y1; color=\"red\", clim=(0, 1))\n    return Plots.scatter!(x2, y2; color=\"blue\", clim=(0, 1))\nend\n\nplot_data()"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Neural Network\n\nThe next step is to define a [feedforward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network) where we express our parameters as distributions, and not single points as with traditional neural networks.\nFor this we will use `Dense` to define liner layers and compose them via `Chain`, both are neural network primitives from Flux.\nThe network `nn_initial` we created has two hidden layers with `tanh` activations and one output layer with sigmoid (`σ`) activation, as shown below.\n\n![nn-diagram](https://user-images.githubusercontent.com/422990/47970321-bd172080-e038-11e8-9c6d-6c2bd790bd8a.png)\n\nThe `nn_initial` is an instance that acts as a function and can take data as inputs and output predictions.\nWe will define distributions on the neural network parameters and use `destructure` from Flux to extract the parameters as `parameters_initial`.\nThe function `destructure` also returns another function `reconstruct` that can take (new) parameters in and return us a neural network instance whose architecture is the same as `nn_initial` but with updated parameters."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Construct a neural network using Flux\nnn_initial = Chain(Dense(2, 3, tanh), Dense(3, 2, tanh), Dense(2, 1, σ))\n\n# Extract weights and a helper function to reconstruct NN from weights\nparameters_initial, reconstruct = Flux.destructure(nn_initial)\n\nlength(parameters_initial) # number of paraemters in NN"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The probabilistic model specification below creates a `parameters` variable, which has IID normal variables. The `parameters` vector represents all parameters of our neural net (weights and biases)."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@model function bayes_nn(xs, ts, nparameters, reconstruct; alpha=0.09)\n    # Create the weight and bias vector.\n    parameters ~ MvNormal(Zeros(nparameters), I / alpha)\n\n    # Construct NN from parameters\n    nn = reconstruct(parameters)\n    # Forward NN to make predictions\n    preds = nn(xs)\n\n    # Observe each prediction.\n    for i in 1:length(ts)\n        ts[i] ~ Bernoulli(preds[i])\n    end\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference can now be performed by calling `sample`. We use the `NUTS` Hamiltonian Monte Carlo sampler here."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Perform inference.\nN = 5000\nch = sample(bayes_nn(hcat(xs...), ts, length(parameters_initial), reconstruct), NUTS(), N);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we extract the parameter samples from the sampled chain as `theta` (this is of size `5000 x 20` where `5000` is the number of iterations and `20` is the number of parameters).\nWe'll use these primarily to determine how good our model's classifier is."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Extract all weight and bias parameters.\ntheta = MCMCChains.group(ch, :parameters).value;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction Visualization\n\nWe can use [MAP estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) to classify our population by using the set of weights that provided the highest log posterior."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# A helper to create NN from weights `theta` and run it through data `x`\nnn_forward(x, theta) = reconstruct(theta)(x)\n\n# Plot the data we have.\nplot_data()\n\n# Find the index that provided the highest log posterior in the chain.\n_, i = findmax(ch[:lp])\n\n# Extract the max row value from i.\ni = i.I[1]\n\n# Plot the posterior distribution with a contour plot\nx1_range = collect(range(-6; stop=6, length=25))\nx2_range = collect(range(-6; stop=6, length=25))\nZ = [nn_forward([x1, x2], theta[i, :])[1] for x1 in x1_range, x2 in x2_range]\ncontour!(x1_range, x2_range, Z)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The contour plot above shows that the MAP method is not too bad at classifying our data.\n\nNow we can visualize our predictions.\n\n$$\np(\\tilde{x} | X, \\alpha) = \\int_{\\theta} p(\\tilde{x} | \\theta) p(\\theta | X, \\alpha) \\approx \\sum_{\\theta \\sim p(\\theta | X, \\alpha)}f_{\\theta}(\\tilde{x})\n$$\n\nThe `nn_predict` function takes the average predicted value from a network parameterized by weights drawn from the MCMC chain."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Return the average predicted value across\n# multiple weights.\nfunction nn_predict(x, theta, num)\n    return mean([nn_forward(x, theta[i, :])[1] for i in 1:10:num])\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we use the `nn_predict` function to predict the value at a sample of points where the `x1` and `x2` coordinates range between -6 and 6. As we can see below, we still have a satisfactory fit to our data, and more importantly, we can also see where the neural network is uncertain about its predictions much easier---those regions between cluster boundaries."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Plot the average prediction.\nplot_data()\n\nn_end = 1500\nx1_range = collect(range(-6; stop=6, length=25))\nx2_range = collect(range(-6; stop=6, length=25))\nZ = [nn_predict([x1, x2], theta, n_end)[1] for x1 in x1_range, x2 in x2_range]\ncontour!(x1_range, x2_range, Z)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we are interested in how the predictive power of our Bayesian neural network evolved between samples. In that case, the following graph displays an animation of the contour plot generated from the network weights in samples 1 to 1,000."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Number of iterations to plot.\nn_end = 500\n\nanim = @gif for i in 1:n_end\n    plot_data()\n    Z = [nn_forward([x1, x2], theta[i, :])[1] for x1 in x1_range, x2 in x2_range]\n    contour!(x1_range, x2_range, Z; title=\"Iteration $i\", clim=(0, 1))\nend every 5"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This has been an introduction to the applications of Turing and Flux in defining Bayesian neural networks."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "if isdefined(Main, :TuringTutorials)\n    Main.TuringTutorials.tutorial_footer(WEAVE_ARGS[:folder], WEAVE_ARGS[:file])\nend"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.9.2"
    },
    "kernelspec": {
      "name": "julia-1.9",
      "display_name": "Julia 1.9.2",
      "language": "julia"
    }
  },
  "nbformat": 4
}
