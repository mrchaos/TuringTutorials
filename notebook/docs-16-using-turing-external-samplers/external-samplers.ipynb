{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using External Samplers on Turing Models\n\n`Turing` provides several wrapped samplers from external sampling libraries, e.g., HMC samplers from `AdvancedHMC`.\nThese wrappers allow new users to seamlessly sample statistical models without leaving `Turing`\nHowever, these wrappers might only sometimes be complete, missing some functionality from the wrapped sampling library.\nMoreover, users might want to use samplers currently not wrapped within `Turing`.\n\nFor these reasons, `Turing` also makes running external samplers on Turing models easy without any necessary modifications or wrapping!\nThroughout, we will use a 10-dimensional Neal's funnel as a running example::"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Import libraries.\nusing Turing, Random, LinearAlgebra\n\nd = 10\n@model function funnel()\n    θ ~ Truncated(Normal(0, 3), -3, 3)\n    z ~ MvNormal(zeros(d - 1), exp(θ) * I)\n    return x ~ MvNormal(z, I)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we sample the model to generate some observations, which we can then condition on."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "(; x) = rand(funnel() | (θ=0,))\nmodel = funnel() | (; x);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Users can use any sampler algorithm to sample this model if it follows the `AbstractMCMC` API.\nBefore discussing how this is done in practice, giving a high-level description of the process is interesting.\nImagine that we created an instance of an external sampler that we will call `spl` such that `typeof(spl)<:AbstractMCMC.AbstractSampler`.\nIn order to avoid type ambiguity within Turing, at the moment it is necessary to declare `spl` as an external sampler to Turing `espl = externalsampler(spl)`, where `externalsampler(s::AbstractMCMC.AbstractSampler)` is a Turing function that types our external sampler adequately.\n\nAn excellent point to start to show how this is done in practice is by looking at the sampling library `AdvancedMH` ((`AdvancedMH`'s GitHub)[[https://github.com/TuringLang/AdvancedMH.jl]) for Metropolis-Hastings (MH) methods.\nLet's say we want to use a random walk Metropolis-Hastings sampler without specifying the proposal distributions.\nThe code below constructs an MH sampler using a multivariate Gaussian distribution with zero mean and unit variance in `d` dimensions as a random walk proposal."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Importing the sampling library\nusing AdvancedMH\nrwmh = AdvancedMH.RWMH(d)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling is then as easy as:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "chain = sample(model, externalsampler(rwmh), 10_000)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Going beyond the Turing API\n\nAs previously mentioned, the Turing wrappers can often limit the capabilities of the sampling libraries they wrap.\n`AdvancedHMC`[^1] ((`AdvancedHMC`'s GitHub)[https://github.com/TuringLang/AdvancedHMC.jl]) is a clear example of this. A common practice when performing HMC is to provide an initial guess for the mass matrix.\nHowever, the native HMC sampler within Turing only allows the user to specify the type of the mass matrix despite the two options being possible within `AdvancedHMC`.\nThankfully, we can use Turing's support for external samplers to define an HMC sampler with a custom mass matrix in `AdvancedHMC` and then use it to sample our Turing model.\n\nWe will use the library `Pathfinder`[^2] ((`Pathfinder`'s GitHub)[https://github.com/mlcolab/Pathfinder.jl]) to construct our estimate of mass matrix.\n`Pathfinder` is a variational inference algorithm that first finds the maximum a posteriori (MAP) estimate of a target posterior distribution and then uses the trace of the optimization to construct a sequence of multivariate normal approximations to the target distribution.\nIn this process, `Pathfinder` computes an estimate of the mass matrix the user can access.\n\nThe code below shows this can be done in practice."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using AdvancedHMC, Pathfinder\n# Running pathfinder\ndraws = 1_000\nresult_multi = multipathfinder(model, draws; nruns=8)\n\n# Estimating the metric\ninv_metric = result_multi.pathfinder_results[1].fit_distribution.Σ\nmetric = DenseEuclideanMetric(Matrix(inv_metric))\n\n# Creating an AdvancedHMC NUTS sampler with the custom metric.\nn_adapts = 1000 # Number of adaptation steps\ntap = 0.9 # Large target acceptance probability to deal with the funnel structure of the posterior\nnuts = AdvancedHMC.NUTS(tap; metric=metric)\n\n# Sample\nchain = sample(model, externalsampler(nuts), 10_000; n_adapts=1_000)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using new inference methods\n\nSo far we have used Turing's support for external samplers to go beyond the capabilities of the wrappers.\nWe want to use this support to employ a sampler not supported within Turing's ecosystem yet.\nWe will use the recently developed Micro-Cannoncial Hamiltonian Monte Carlo (MCHMC) sampler to showcase this.\nMCHMC[^3,^4] ((MCHMC's GitHub)[https://github.com/JaimeRZP/MicroCanonicalHMC.jl]) is HMC sampler that uses one single Hamiltonian energy level to explore the whole parameter space.\nThis is achieved by simulating the dynamics of a microcanonical Hamiltonian with an additional noise term to ensure ergodicity.\n\nUsing this as well as other inference methods outside the Turing ecosystem is as simple as executing the code shown below:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using MicroCanonicalHMC\n# Create MCHMC sampler\nn_adapts = 1_000 # adaptation steps\ntev = 0.01 # target energy variance\nmchmc = MCHMC(n_adapts, tev; adaptive=true)\n\n# Sample\nchain = sample(model, externalsampler(mchmc), 10_000)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only requirement to work with `externalsampler` is that the provided `sampler` must implement the AbstractMCMC.jl-interface [INSERT LINK] for a `model` of type `AbstractMCMC.LogDensityModel` [INSERT LINK].\n\nAs previously stated, in order to use external sampling libraries within `Turing` they must follow the `AbstractMCMC` API.\nIn this section, we will briefly dwell on what this entails.\nFirst and foremost, the sampler should be a subtype of `AbstractMCMC.AbstractSampler`.\nSecond, the stepping function of the MCMC algorithm must be made defined using `AbstractMCMC.step` and follow the structure below:\n\n```\n# First step\nfunction AbstractMCMC.step{T<:AbstractMCMC.AbstractSampler}(\n    rng::Random.AbstractRNG,\n    model::AbstractMCMC.LogDensityModel,\n    spl::T;\n    kwargs...,\n)\n    [...]\n    return transition, sample\nend\n\n# N+1 step\nfunction AbstractMCMC.step{T<:AbstractMCMC.AbstractSampler}(\n    rng::Random.AbstractRNG,\n    model::AbstractMCMC.LogDensityModel,\n    sampler::T,\n    state;\n    kwargs...,\n) \n    [...]\n    return transition, sample\nend\n```\n\nThere are several characteristics to note in these functions:\n\n  - There must be two `step` functions:\n    \n      + A function that performs the first step and initializes the sampler.\n      + A function that performs the following steps and takes an extra input, `state`, which carries the initialization information.\n\n  - The functions must follow the displayed signatures.\n  - The output of the functions must be a transition, the current state of the sampler, and a sample, what is saved to the MCMC chain.\n\nThe last requirement is that the transition must be structured with a field `θ` which contains the values of the parameters of the model for said transition.\nThis allows `Turing` to seamlessly extract the parameter values at each step of the chain when bundling the chains.\nNote that if the external sampler produces transitions that Turing cannot parse the bundling of the samples will be different or fail.\n\nFor practical examples of how to adapt a sampling library to the `AbstractMCMC` interface, the readers can consult the following libraries:\n\n  - (AdvancedMH)[https://github.com/TuringLang/AdvancedMH.jl/blob/458a602ac32a8514a117d4c671396a9ba8acbdab/src/mh-core.jl#L73-L115]\n  - (AdvancedHMC)[https://github.com/TuringLang/AdvancedHMC.jl/blob/762e55f894d142495a41a6eba0eed9201da0a600/src/abstractmcmc.jl#L102-L170]\n  - (MicroCanonicalHMC)[https://github.com/JaimeRZP/MicroCanonicalHMC.jl/blob/master/src/abstractmcmc.jl] within `MicroCanonicalHMC`.\n\n# Refences\n\n[^1]: Xu et al, (AdvancedHMC.jl: A robust, modular and efficient implementation of advanced HMC algorithms)[http://proceedings.mlr.press/v118/xu20a/xu20a.pdf], 2019\n[^2]: Zhang et al, (Pathfinder: Parallel quasi-Newton variational inference)[https://arxiv.org/abs/2108.03782], 2021\n[^3]: Robnik et al, (Microcanonical Hamiltonian Monte Carlo)[https://arxiv.org/abs/2212.08549], 2022\n[^4]: Robnik and Seljak, (Langevine Hamiltonian Monte Carlo)[https://arxiv.org/abs/2303.18221], 2023"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.9.2"
    },
    "kernelspec": {
      "name": "julia-1.9",
      "display_name": "Julia 1.9.2",
      "language": "julia"
    }
  },
  "nbformat": 4
}
