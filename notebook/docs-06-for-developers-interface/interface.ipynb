{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The sampling interface\n\nTuring implements a sampling interface (hosted at\n[AbstractMCMC](https://github.com/TuringLang/AbstractMCMC.jl)) that is intended to provide\na common framework for Markov chain Monte Carlo samplers. The interface presents several\nstructures and functions that one needs to overload in order to implement an\ninterface-compatible sampler.\n\nThis guide will demonstrate how to implement the interface without Turing.\n\n## Interface overview\n\nAny implementation of an inference method that uses the AbstractMCMC interface should\nimplement a subset of the following types and functions:\n\n 1. A subtype of `AbstractSampler`, defined as a mutable struct containing state information or sampler parameters.\n 2. A function `sample_init!` which performs any necessary set-up (default: do not perform any set-up).\n 3. A function `step!` which returns a transition that represents a single draw from the sampler.\n 4. A function `transitions_init` which returns a container for the transitions obtained from the sampler\n    (default: return a `Vector{T}` of length `N` where `T` is the type of the transition obtained in the first step and `N` is the number of requested samples).\n 5. A function `transitions_save!` which saves transitions to the container (default: save the transition of iteration `i`\n    at position `i` in the vector of transitions).\n 6. A function `sample_end!` which handles any sampler wrap-up (default: do not perform any wrap-up).\n 7. A function `bundle_samples` which accepts the container of transitions and returns a collection of samples\n    (default: return the vector of transitions).\n\nThe interface methods with exclamation points are those that are intended to allow for\nstate mutation. Any mutating function is meant to allow mutation where needed -- you might\nuse:\n\n  - `sample_init!` to run some kind of sampler preparation, before sampling begins. This\n    could mutate a sampler's state.\n  - `step!` might mutate a sampler flag after each sample.\n  - `sample_end!` contains any wrap-up you might need to do. If you were sampling in a\n    transformed space, this might be where you convert everything back to a constrained space.\n\n## Why do you have an interface?\n\nThe motivation for the interface is to allow Julia's fantastic probabilistic programming\nlanguage community to have a set of standards and common implementations so we can all\nthrive together. Markov chain Monte Carlo methods tend to have a very similar framework to\none another, and so a common interface should help more great inference methods built in\nsingle-purpose packages to experience more use among the community.\n\n## Implementing Metropolis-Hastings without Turing\n\n[Metropolis-Hastings](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) is often the\nfirst sampling method that people are exposed to. It is a very straightforward algorithm\nand is accordingly the easiest to implement, so it makes for a good example. In this\nsection, you will learn how to use the types and functions listed above to implement the\nMetropolis-Hastings sampler using the MCMC interface.\n\nThe full code for this implementation is housed in\n[AdvancedMH.jl](https://github.com/TuringLang/AdvancedMH.jl).\n\n### Imports\n\nLet's begin by importing the relevant libraries. We'll import `AbstractMCMC`, which contains\nthe interface framework we'll fill out. We also need `Distributions` and `Random`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Import the relevant libraries.\nusing AbstractMCMC: AbstractMCMC\nusing Distributions\nusing Random"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "An interface extension (like the one we're writing right now) typically requires that you overload or implement several functions. Specifically, you should `import` the functions you intend to overload. This next code block accomplishes that.\n\nFrom `Distributions`, we need `Sampleable`, `VariateForm`, and `ValueSupport`, three abstract types that define a distribution. Models in the interface are assumed to be subtypes of `Sampleable{VariateForm, ValueSupport}`. In this section our model is going be be extremely simple, so we will not end up using these except to make sure that the inference functions are dispatching correctly.\n\n### Sampler\n\nLet's begin our sampler definition by defining a sampler called `MetropolisHastings` which\nis a subtype of `AbstractSampler`. Correct typing is very important for proper interface\nimplementation -- if you are missing a subtype, your method may not be dispatched to when\nyou call `sample`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Define a sampler type.\nstruct MetropolisHastings{T,D} <: AbstractMCMC.AbstractSampler\n    init_θ::T\n    proposal::D\nend\n\n# Default constructors.\nMetropolisHastings(init_θ::Real) = MetropolisHastings(init_θ, Normal(0, 1))\nfunction MetropolisHastings(init_θ::Vector{<:Real})\n    return MetropolisHastings(init_θ, MvNormal(zero(init_θ), I))\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, we have defined a sampler that stores the initial parameterization of the prior,\nand a distribution object from which proposals are drawn. You can have a struct that has no\nfields, and simply use it for dispatching onto the relevant functions, or you can store a\nlarge amount of state information in your sampler.\n\nThe general intuition for what to store in your sampler struct is that anything you may\nneed to perform inference between samples but you don't want to store in a transition\nshould go into the sampler struct. It's the only way you can carry non-sample related state\ninformation between `step!` calls.\n\n### Model\n\nNext, we need to have a model of some kind. A model is a struct that's a subtype of\n`AbstractModel` that contains whatever information is necessary to perform inference on\nyour problem. In our case we want to know the mean and variance parameters for a standard\nNormal distribution, so we can keep our model to the log density of a Normal.\n\nNote that we only have to do this because we are not yet integrating the sampler with Turing\n-- Turing has a very sophisticated modelling engine that removes the need to define custom\nmodel structs."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Define a model type. Stores the log density function.\nstruct DensityModel{F<:Function} <: AbstractMCMC.AbstractModel\n    ℓπ::F\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transition\n\nThe next step is to define some transition which we will return from each `step!` call.\nWe'll keep it simple by just defining a wrapper struct that contains the parameter draws\nand the log density of that draw:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Create a very basic Transition type, only stores the \n# parameter draws and the log probability of the draw.\nstruct Transition{T,L}\n    θ::T\n    lp::L\nend\n\n# Store the new draw and its log density.\nTransition(model::DensityModel, θ) = Transition(θ, ℓπ(model, θ))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Transition` can now store any type of parameter, whether it's a vector of draws from\nmultiple parameters or a single univariate draw.\n\n### Metropolis-Hastings\n\nNow it's time to get into the actual inference. We've defined all of the core pieces we\nneed, but we need to implement the `step!` function which actually performs inference.\n\nAs a refresher, Metropolis-Hastings implements a very basic algorithm:\n\n 1. Pick some initial state, ``\\theta_0``.\n\n 2. For ``t`` in ``[1,N],`` do\n    \n      + Generate a proposal parameterization ``\\theta^\\prime_t \\sim q(\\theta^\\prime_t \\mid \\theta_{t-1}).``\n      + Calculate the acceptance probability, ``\\alpha = \\text{min}\\left[1,\\frac{\\pi(\\theta'_t)}{\\pi(\\theta_{t-1})} \\frac{q(\\theta_{t-1} \\mid \\theta'_t)}{q(\\theta'_t \\mid \\theta_{t-1})}) \\right].``\n      + If ``U \\le \\alpha`` where ``U \\sim [0,1],`` then ``\\theta_t = \\theta'_t.`` Otherwise, ``\\theta_t = \\theta_{t-1}.``\n\nOf course, it's much easier to do this in the log space, so the acceptance probability is\nmore commonly written as\n\n```math\n\\log \\alpha = \\min\\left[0, \\log \\pi(\\theta'_t) - \\log \\pi(\\theta_{t-1}) + \\log q(\\theta_{t-1} \\mid \\theta^\\prime_t) - \\log q(\\theta\\prime_t \\mid \\theta_{t-1}) \\right].\n```\n\nIn interface terms, we should do the following:\n\n 1. Make a new transition containing a proposed sample.\n 2. Calculate the acceptance probability.\n 3. If we accept, return the new transition, otherwise, return the old one.\n\n### Steps\n\nThe `step!` function is the function that performs the bulk of your inference. In our case,\nwe will implement two `step!` functions -- one for the very first iteration, and one for\nevery subsequent iteration."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Define the first step! function, which is called at the \n# beginning of sampling. Return the initial parameter used\n# to define the sampler.\nfunction AbstractMCMC.step!(\n    rng::AbstractRNG,\n    model::DensityModel,\n    spl::MetropolisHastings,\n    N::Integer,\n    ::Nothing;\n    kwargs...,\n)\n    return Transition(model, spl.init_θ)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first `step!` function just packages up the initial parameterization inside the\nsampler, and returns it. We implicitly accept the very first parameterization.\n\nThe other `step!` function performs the usual steps from Metropolis-Hastings. Included are\nseveral helper functions, `proposal` and `q`, which are designed to replicate the functions\nin the pseudocode above.\n\n  - `proposal` generates a new proposal in the form of a `Transition`, which can be\n    univariate if the value passed in is univariate, or it can be multivariate if the\n    `Transition` given is multivariate. Proposals use a basic `Normal` or `MvNormal` proposal\n    distribution.\n  - `q` returns the log density of one parameterization conditional on another, according to\n    the proposal distribution.\n  - `step!` generates a new proposal, checks the acceptance probability, and then returns\n    either the previous transition or the proposed transition."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Define a function that makes a basic proposal depending on a univariate\n# parameterization or a multivariate parameterization.\nfunction propose(spl::MetropolisHastings, model::DensityModel, θ::Real)\n    return Transition(model, θ + rand(spl.proposal))\nend\nfunction propose(spl::MetropolisHastings, model::DensityModel, θ::Vector{<:Real})\n    return Transition(model, θ + rand(spl.proposal))\nend\nfunction propose(spl::MetropolisHastings, model::DensityModel, t::Transition)\n    return propose(spl, model, t.θ)\nend\n\n# Calculates the probability `q(θ|θcond)`, using the proposal distribution `spl.proposal`.\nq(spl::MetropolisHastings, θ::Real, θcond::Real) = logpdf(spl.proposal, θ - θcond)\nfunction q(spl::MetropolisHastings, θ::Vector{<:Real}, θcond::Vector{<:Real})\n    return logpdf(spl.proposal, θ - θcond)\nend\nq(spl::MetropolisHastings, t1::Transition, t2::Transition) = q(spl, t1.θ, t2.θ)\n\n# Calculate the density of the model given some parameterization.\nℓπ(model::DensityModel, θ) = model.ℓπ(θ)\nℓπ(model::DensityModel, t::Transition) = t.lp\n\n# Define the other step function. Returns a Transition containing\n# either a new proposal (if accepted) or the previous proposal \n# (if not accepted).\nfunction AbstractMCMC.step!(\n    rng::AbstractRNG,\n    model::DensityModel,\n    spl::MetropolisHastings,\n    ::Integer,\n    θ_prev::Transition;\n    kwargs...,\n)\n    # Generate a new proposal.\n    θ = propose(spl, model, θ_prev)\n\n    # Calculate the log acceptance probability.\n    α = ℓπ(model, θ) - ℓπ(model, θ_prev) + q(spl, θ_prev, θ) - q(spl, θ, θ_prev)\n\n    # Decide whether to return the previous θ or the new one.\n    if log(rand(rng)) < min(α, 0.0)\n        return θ\n    else\n        return θ_prev\n    end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chains\n\nIn the default implementation, `sample` just returns a vector of all transitions. If\ninstead you would like to obtain a `Chains` object (e.g., to simplify downstream analysis),\nyou have to implement the `bundle_samples` function as well. It accepts the vector of\ntransitions and returns a collection of samples. Fortunately, our `Transition` is\nincredibly simple, and we only need to build a little bit of functionality to accept custom\nparameter names passed in by the user."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# A basic chains constructor that works with the Transition struct we defined.\nfunction AbstractMCMC.bundle_samples(\n    rng::AbstractRNG,\n    ℓ::DensityModel,\n    s::MetropolisHastings,\n    N::Integer,\n    ts::Vector{<:Transition},\n    chain_type::Type{Any};\n    param_names=missing,\n    kwargs...,\n)\n    # Turn all the transitions into a vector-of-vectors.\n    vals = copy(reduce(hcat, [vcat(t.θ, t.lp) for t in ts])')\n\n    # Check if we received any parameter names.\n    if ismissing(param_names)\n        param_names = [\"Parameter $i\" for i in 1:(length(first(vals)) - 1)]\n    end\n\n    # Add the log density field to the parameter names.\n    push!(param_names, \"lp\")\n\n    # Bundle everything up and return a Chains struct.\n    return Chains(vals, param_names, (internals=[\"lp\"],))\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "All done!\n\nYou can even implement different output formats by implementing `bundle_samples` for\ndifferent `chain_type`s, which can be provided as keyword argument to `sample`. As default\n`sample` uses `chain_type = Any`.\n\n### Testing the implementation\n\nNow that we have all the pieces, we should test the implementation by defining a model to\ncalculate the mean and variance parameters of a Normal distribution. We can do this by\nconstructing a target density function, providing a sample of data, and then running the\nsampler with `sample`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Generate a set of data from the posterior we want to estimate.\ndata = rand(Normal(5, 3), 30)\n\n# Define the components of a basic model.\ninsupport(θ) = θ[2] >= 0\ndist(θ) = Normal(θ[1], θ[2])\ndensity(θ) = insupport(θ) ? sum(logpdf.(dist(θ), data)) : -Inf\n\n# Construct a DensityModel.\nmodel = DensityModel(density)\n\n# Set up our sampler with initial parameters.\nspl = MetropolisHastings([0.0, 0.0])\n\n# Sample from the posterior.\nchain = sample(model, spl, 100000; param_names=[\"μ\", \"σ\"])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all the interface functions have been extended properly, you should get an output from\n`display(chain)` that looks something like this:\n\n```\nObject of type Chains, with data of type 100000×3×1 Array{Float64,3}\n\nIterations        = 1:100000\nThinning interval = 1\nChains            = 1\nSamples per chain = 100000\ninternals         = lp\nparameters        = μ, σ\n\n2-element Array{ChainDataFrame,1}\n\nSummary Statistics\n\n│ Row │ parameters │ mean    │ std      │ naive_se   │ mcse       │ ess     │ r_hat   │\n│     │ Symbol     │ Float64 │ Float64  │ Float64    │ Float64    │ Any     │ Any     │\n├─────┼────────────┼─────────┼──────────┼────────────┼────────────┼─────────┼─────────┤\n│ 1   │ μ          │ 5.33157 │ 0.854193 │ 0.0027012  │ 0.00893069 │ 8344.75 │ 1.00009 │\n│ 2   │ σ          │ 4.54992 │ 0.632916 │ 0.00200146 │ 0.00534942 │ 14260.8 │ 1.00005 │\n\nQuantiles\n\n│ Row │ parameters │ 2.5%    │ 25.0%   │ 50.0%   │ 75.0%   │ 97.5%   │\n│     │ Symbol     │ Float64 │ Float64 │ Float64 │ Float64 │ Float64 │\n├─────┼────────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n│ 1   │ μ          │ 3.6595  │ 4.77754 │ 5.33182 │ 5.89509 │ 6.99651 │\n│ 2   │ σ          │ 3.5097  │ 4.09732 │ 4.47805 │ 4.93094 │ 5.96821 │\n```\n\nIt looks like we're extremely close to our true parameters of `Normal(5,3)`, though with a\nfairly high variance due to the low sample size.\n\n## Conclusion\n\nWe've seen how to implement the sampling interface for general projects. Turing's interface\nmethods are ever-evolving, so please open an issue at\n[AbstractMCMC](https://github.com/TuringLang/AbstractMCMC.jl) with feature requests or\nproblems."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.9.2"
    },
    "kernelspec": {
      "name": "julia-1.9",
      "display_name": "Julia 1.9.2",
      "language": "julia"
    }
  },
  "nbformat": 4
}
