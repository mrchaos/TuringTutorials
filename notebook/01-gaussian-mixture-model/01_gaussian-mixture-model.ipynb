{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The following tutorial illustrates the use of Turing for clustering data using a Bayesian mixture model.\nThe aim of this task is to infer a latent grouping (hidden structure) from unlabelled data.\n\n## Synthetic Data\n\nWe generate a synthetic dataset of $N = 60$ two-dimensional points $x_i \\in \\mathbb{R}^2$ drawn from a Gaussian mixture model.\nFor simplicity, we use $K = 2$ clusters with\n\n  - equal weights, i.e., we use mixture weights $w = [0.5, 0.5]$, and\n  - isotropic Gaussian distributions of the points in each cluster.\n\nMore concretely, we use the Gaussian distributions $\\mathcal{N}([\\mu_k, \\mu_k]^\\mathsf{T}, I)$ with parameters $\\mu_1 = -3.5$ and $\\mu_2 = 0.5$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Distributions\nusing FillArrays\nusing StatsPlots\n\nusing LinearAlgebra\nusing Random\n\n# Set a random seed.\nRandom.seed!(3)\n\n# Define Gaussian mixture model.\nw = [0.5, 0.5]\nμ = [-3.5, 0.5]\nmixturemodel = MixtureModel([MvNormal(Fill(μₖ, 2), I) for μₖ in μ], w)\n\n# We draw the data points.\nN = 60\nx = rand(mixturemodel, N);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following plot shows the dataset."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "scatter(x[1, :], x[2, :]; legend=false, title=\"Synthetic Dataset\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussian Mixture Model in Turing\n\nWe are interested in recovering the grouping from the dataset.\nMore precisely, we want to infer the mixture weights, the parameters $\\mu_1$ and $\\mu_2$, and the assignment of each datum to a cluster for the generative Gaussian mixture model.\n\nIn a Bayesian Gaussian mixture model with $K$ components each data point $x_i$ ($i = 1,\\ldots,N$) is generated according to the following generative process.\nFirst we draw the model parameters, i.e., in our example we draw parameters $\\mu_k$ for the mean of the isotropic normal distributions and the mixture weights $w$ of the $K$ clusters.\nWe use standard normal distributions as priors for $\\mu_k$ and a Dirichlet distribution with parameters $\\alpha_1 = \\cdots = \\alpha_K = 1$ as prior for $w$:\n$$\n\\begin{aligned}\n\\mu_k &\\sim \\mathcal{N}(0, 1) \\qquad (k = 1,\\ldots,K)\\\\\nw &\\sim \\operatorname{Dirichlet}(\\alpha_1, \\ldots, \\alpha_K)\n\\end{aligned}\n$$\nAfter having constructed all the necessary model parameters, we can generate an observation by first selecting one of the clusters\n$$\nz_i \\sim \\operatorname{Categorical}(w) \\qquad (i = 1,\\ldots,N),\n$$\nand then drawing the datum accordingly, i.e., in our example drawing\n$$\nx_i \\sim \\mathcal{N}([\\mu_{z_i}, \\mu_{z_i}]^\\mathsf{T}, I) \\qquad (i=1,\\ldots,N).\n$$\nFor more details on Gaussian mixture models, we refer to Christopher M. Bishop, *Pattern Recognition and Machine Learning*, Section 9.\n\nWe specify the model with Turing."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Turing\n\n@model function gaussian_mixture_model(x)\n    # Draw the parameters for each of the K=2 clusters from a standard normal distribution.\n    K = 2\n    μ ~ MvNormal(Zeros(K), I)\n\n    # Draw the weights for the K clusters from a Dirichlet distribution with parameters αₖ = 1.\n    w ~ Dirichlet(K, 1.0)\n    # Alternatively, one could use a fixed set of weights.\n    # w = fill(1/K, K)\n\n    # Construct categorical distribution of assignments.\n    distribution_assignments = Categorical(w)\n\n    # Construct multivariate normal distributions of each cluster.\n    D, N = size(x)\n    distribution_clusters = [MvNormal(Fill(μₖ, D), I) for μₖ in μ]\n\n    # Draw assignments for each datum and generate it from the multivariate normal distribution.\n    k = Vector{Int}(undef, N)\n    for i in 1:N\n        k[i] ~ distribution_assignments\n        x[:, i] ~ distribution_clusters[k[i]]\n    end\n\n    return k\nend\n\nmodel = gaussian_mixture_model(x);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run a MCMC simulation to obtain an approximation of the posterior distribution of the parameters $\\mu$ and $w$ and assignments $k$.\nWe use a `Gibbs` sampler that combines a [particle Gibbs](https://www.stats.ox.ac.uk/%7Edoucet/andrieu_doucet_holenstein_PMCMC.pdf) sampler for the discrete parameters (assignments $k$) and a Hamiltonion Monte Carlo sampler for the continuous parameters ($\\mu$ and $w$).\nWe generate multiple chains in parallel using multi-threading."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "sampler = Gibbs(PG(100, :k), HMC(0.05, 10, :μ, :w))\nnsamples = 100\nnchains = 3\nchains = sample(model, sampler, MCMCThreads(), nsamples, nchains);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "let\n    # Verify that the output of the chain is as expected.\n    for i in MCMCChains.chains(chains)\n        # μ[1] and μ[2] can switch places, so we sort the values first.\n        chain = Array(chains[:, [\"μ[1]\", \"μ[2]\"], i])\n        μ_mean = vec(mean(chain; dims=1))\n        @assert isapprox(sort(μ_mean), μ; rtol=0.1) \"Difference between estimated mean of μ ($(sort(μ_mean))) and data-generating μ ($μ) unexpectedly large!\"\n    end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferred Mixture Model\n\nAfter sampling we can visualize the trace and density of the parameters of interest.\n\nWe consider the samples of the location parameters $\\mu_1$ and $\\mu_2$ for the two clusters."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(chains[[\"μ[1]\", \"μ[2]\"]]; colordim=:parameter, legend=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can happen that the modes of $\\mu_1$ and $\\mu_2$ switch between chains.\nFor more information see the [Stan documentation](https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html) for potential solutions.\n\nWe also inspect the samples of the mixture weights $w$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(chains[[\"w[1]\", \"w[2]\"]]; colordim=:parameter, legend=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following, we just use the first chain to ensure the validity of our inference."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "chain = chains[:, :, 1];"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the distributions of the samples for the parameters $\\mu_1$, $\\mu_2$, $w_1$, and $w_2$ are unimodal, we can safely visualize the density region of our model using the average values."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Model with mean of samples as parameters.\nμ_mean = [mean(chain, \"μ[$i]\") for i in 1:2]\nw_mean = [mean(chain, \"w[$i]\") for i in 1:2]\nmixturemodel_mean = MixtureModel([MvNormal(Fill(μₖ, 2), I) for μₖ in μ_mean], w_mean)\n\ncontour(\n    range(-7.5, 3; length=1_000),\n    range(-6.5, 3; length=1_000),\n    (x, y) -> logpdf(mixturemodel_mean, [x, y]);\n    widen=false,\n)\nscatter!(x[1, :], x[2, :]; legend=false, title=\"Synthetic Dataset\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferred Assignments\n\nFinally, we can inspect the assignments of the data points inferred using Turing.\nAs we can see, the dataset is partitioned into two distinct groups."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "assignments = [mean(chain, \"k[$i]\") for i in 1:N]\nscatter(\n    x[1, :],\n    x[2, :];\n    legend=false,\n    title=\"Assignments on Synthetic Dataset\",\n    zcolor=assignments,\n)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "if isdefined(Main, :TuringTutorials)\n    Main.TuringTutorials.tutorial_footer(WEAVE_ARGS[:folder], WEAVE_ARGS[:file])\nend"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.9.2"
    },
    "kernelspec": {
      "name": "julia-1.9",
      "display_name": "Julia 1.9.2",
      "language": "julia"
    }
  },
  "nbformat": 4
}
