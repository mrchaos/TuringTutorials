{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Tips\n\nThis section briefly summarises a few common techniques to ensure good performance when using Turing.\nWe refer to [the Julia documentation](https://docs.julialang.org/en/v1/manual/performance-tips/index.html) for general techniques to ensure good performance of Julia programs.\n\n## Use multivariate distributions\n\nIt is generally preferable to use multivariate distributions if possible.\n\nThe following example:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Turing\n@model function gmodel(x)\n    m ~ Normal()\n    for i in 1:length(x)\n        x[i] ~ Normal(m, 0.2)\n    end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "can be directly expressed more efficiently using a simple transformation:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using FillArrays\n\n@model function gmodel(x)\n    m ~ Normal()\n    return x ~ MvNormal(Fill(m, length(x)), 0.04 * I)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose your AD backend\n\nTuring currently provides support for two different automatic differentiation (AD) backends.\nGenerally, try to use `:forwarddiff` for models with few parameters and `:reversediff`, `:tracker` or `:zygote` for models with large parameter vectors or linear algebra operations. See [Automatic Differentiation](autodiff) for details.\n\n## Special care for `:tracker` and `:zygote`\n\nIn case of `:tracker` and `:zygote`, it is necessary to avoid loops for now.\nThis is mainly due to the reverse-mode AD backends `Tracker` and `Zygote` which are inefficient for such cases. `ReverseDiff` does better but vectorized operations will still perform better.\n\nAvoiding loops can be done using `filldist(dist, N)` and `arraydist(dists)`. `filldist(dist, N)` creates a multivariate distribution that is composed of `N` identical and independent copies of the univariate distribution `dist` if `dist` is univariate, or it creates a matrix-variate distribution composed of `N` identical and independent copies of the multivariate distribution `dist` if `dist` is multivariate. `filldist(dist, N, M)` can also be used to create a matrix-variate distribution from a univariate distribution `dist`.  `arraydist(dists)` is similar to `filldist` but it takes an array of distributions `dists` as input. Writing a [custom distribution](advanced) with a custom adjoint is another option to avoid loops.\n\n## Ensure that types in your model can be inferred\n\nFor efficient gradient-based inference, e.g. using HMC, NUTS or ADVI, it is important to ensure the types in your model can be inferred.\n\nThe following example with abstract types"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@model function tmodel(x, y)\n    p, n = size(x)\n    params = Vector{Real}(undef, n)\n    for i in 1:n\n        params[i] ~ truncated(Normal(), 0, Inf)\n    end\n\n    a = x * params\n    return y ~ MvNormal(a, I)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "can be transformed into the following representation with concrete types:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@model function tmodel(x, y, ::Type{T}=Float64) where {T}\n    p, n = size(x)\n    params = Vector{T}(undef, n)\n    for i in 1:n\n        params[i] ~ truncated(Normal(), 0, Inf)\n    end\n\n    a = x * params\n    return y ~ MvNormal(a, I)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you could use `filldist` in this example:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@model function tmodel(x, y)\n    params ~ filldist(truncated(Normal(), 0, Inf), size(x, 2))\n    a = x * params\n    return y ~ MvNormal(a, I)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that you can use `@code_warntype` to find types in your model definition that the compiler cannot infer.\nThey are marked in red in the Julia REPL.\n\nFor example, consider the following simple program:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@model function tmodel(x)\n    p = Vector{Real}(undef, 1)\n    p[1] ~ Normal()\n    p = p .+ 1\n    return x ~ Normal(p[1])\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Random\n\nmodel = tmodel(1.0)\n\n@code_warntype model.f(\n    model,\n    Turing.VarInfo(model),\n    Turing.SamplingContext(\n        Random.default_rng(), Turing.SampleFromPrior(), Turing.DefaultContext()\n    ),\n    model.args...,\n)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "to inspect type inference in the model.\n\n## Reuse Computations in Gibbs Sampling\n\nOften when performing Gibbs sampling, one can save computational time by caching the output of expensive functions. The cached values can then be reused in future Gibbs sub-iterations which do not change the inputs to this expensive function. For example in the following model"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@model function demo(x)\n    a ~ Gamma()\n    b ~ Normal()\n    c = function1(a)\n    d = function2(b)\n    return x .~ Normal(c, d)\nend\nalg = Gibbs(MH(:a), MH(:b))\nsample(demo(zeros(10)), alg, 1000)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "when only updating `a` in a Gibbs sub-iteration, keeping `b` the same, the value of `d` doesn't change. And when only updating `b`, the value of `c` doesn't change. However, if `function1` and `function2` are expensive and are both run in every Gibbs sub-iteration, a lot of time would be spent computing values that we already computed before. Such a problem can be overcome using `Memoization.jl`. Memoizing a function lets us store and reuse the output of the function for every input it is called with. This has a slight time overhead but for expensive functions, the savings will be far greater.\n\nTo use `Memoization.jl`, simply define memoized versions of `function1` and `function2` as such:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Memoization\n\n@memoize memoized_function1(args...) = function1(args...)\n@memoize memoized_function2(args...) = function2(args...)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then define the `Turing` model using the new functions as such:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@model function demo(x)\n    a ~ Gamma()\n    b ~ Normal()\n    c = memoized_function1(a)\n    d = memoized_function2(b)\n    return x .~ Normal(c, d)\nend"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.9.2"
    },
    "kernelspec": {
      "name": "julia-1.9",
      "display_name": "Julia 1.9.2",
      "language": "julia"
    }
  },
  "nbformat": 4
}
